{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSGMRRw-1WMa"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWXlrR2X1wvy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Increase precision of presented data for better side-by-side comparison\n",
        "pd.set_option(\"display.precision\", 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o1mRcZvM177G",
        "outputId": "9ffa2197-a9d9-47c5-cb78-ef09a1c3fb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-3-c3ba193903e1>:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version:  2.14.0\n",
            "Hub version:  0.15.0\n",
            "Eager mode:  True\n",
            "GPU is available\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/COURSE PROJECT\n",
            "Found 202 images belonging to 8 classes.\n",
            "Found 828 images belonging to 8 classes.\n",
            "Image batch shape:  (32, 224, 224, 3)\n",
            "Label batch shape:  (32, 8)\n",
            "['Gpt Reading' 'Gpt Generating' 'Gpt Writing' 'Jupyter Error'\n",
            " 'Jupyter Running' 'Excel' 'Jupyter Success' 'Other']\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 2048)              42626560  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 16392     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42642952 (162.67 MB)\n",
            "Trainable params: 16392 (64.03 KB)\n",
            "Non-trainable params: 42626560 (162.61 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "26/26 [==============================] - 280s 10s/step - loss: 1.9477 - acc: 0.3720 - val_loss: 1.1614 - val_acc: 0.6485\n",
            "Epoch 2/20\n",
            "26/26 [==============================] - 11s 424ms/step - loss: 0.8592 - acc: 0.7597 - val_loss: 0.7055 - val_acc: 0.7673\n",
            "Epoch 3/20\n",
            "26/26 [==============================] - 9s 354ms/step - loss: 0.5423 - acc: 0.8527 - val_loss: 0.5439 - val_acc: 0.8465\n",
            "Epoch 4/20\n",
            "26/26 [==============================] - 10s 391ms/step - loss: 0.4326 - acc: 0.8804 - val_loss: 0.4713 - val_acc: 0.8515\n",
            "Epoch 5/20\n",
            "26/26 [==============================] - 10s 396ms/step - loss: 0.3493 - acc: 0.9106 - val_loss: 0.4277 - val_acc: 0.8663\n",
            "Epoch 6/20\n",
            "26/26 [==============================] - 11s 412ms/step - loss: 0.3118 - acc: 0.9191 - val_loss: 0.3770 - val_acc: 0.8812\n",
            "Epoch 7/20\n",
            "26/26 [==============================] - 10s 397ms/step - loss: 0.2855 - acc: 0.9300 - val_loss: 0.3701 - val_acc: 0.8713\n",
            "Epoch 8/20\n",
            "26/26 [==============================] - 11s 403ms/step - loss: 0.2594 - acc: 0.9300 - val_loss: 0.3628 - val_acc: 0.9010\n",
            "Epoch 9/20\n",
            "26/26 [==============================] - 9s 352ms/step - loss: 0.2332 - acc: 0.9336 - val_loss: 0.3498 - val_acc: 0.8713\n",
            "Epoch 10/20\n",
            "26/26 [==============================] - 11s 412ms/step - loss: 0.2230 - acc: 0.9432 - val_loss: 0.3142 - val_acc: 0.8812\n",
            "Epoch 11/20\n",
            "26/26 [==============================] - 10s 384ms/step - loss: 0.2169 - acc: 0.9384 - val_loss: 0.2999 - val_acc: 0.8960\n",
            "Epoch 12/20\n",
            "26/26 [==============================] - 10s 362ms/step - loss: 0.2069 - acc: 0.9420 - val_loss: 0.3045 - val_acc: 0.8960\n",
            "Epoch 13/20\n",
            "26/26 [==============================] - 11s 408ms/step - loss: 0.1887 - acc: 0.9517 - val_loss: 0.2868 - val_acc: 0.8911\n",
            "Epoch 14/20\n",
            "26/26 [==============================] - 10s 390ms/step - loss: 0.1983 - acc: 0.9372 - val_loss: 0.2963 - val_acc: 0.9010\n",
            "Epoch 15/20\n",
            "26/26 [==============================] - 10s 377ms/step - loss: 0.1713 - acc: 0.9493 - val_loss: 0.2707 - val_acc: 0.9010\n",
            "Epoch 16/20\n",
            "26/26 [==============================] - 10s 398ms/step - loss: 0.1839 - acc: 0.9432 - val_loss: 0.3046 - val_acc: 0.8861\n",
            "Epoch 17/20\n",
            "26/26 [==============================] - 10s 373ms/step - loss: 0.1812 - acc: 0.9457 - val_loss: 0.2511 - val_acc: 0.9010\n",
            "Epoch 18/20\n",
            "26/26 [==============================] - 11s 403ms/step - loss: 0.1807 - acc: 0.9396 - val_loss: 0.2872 - val_acc: 0.9059\n",
            "Epoch 19/20\n",
            "26/26 [==============================] - 9s 353ms/step - loss: 0.1758 - acc: 0.9457 - val_loss: 0.2436 - val_acc: 0.9059\n",
            "Epoch 20/20\n",
            "26/26 [==============================] - 10s 384ms/step - loss: 0.1639 - acc: 0.9493 - val_loss: 0.2592 - val_acc: 0.9059\n",
            "7/7 [==============================] - 2s 233ms/step - loss: 0.2592 - acc: 0.9059\n",
            "Final loss: 0.26\n",
            "Final accuracy: 90.59%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c3ba193903e1>\u001b[0m in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# to reload your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mshoe_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                             return saved_model_load.load(\n\u001b[0m\u001b[1;32m    240\u001b[0m                                 \u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Trying to load ShardedVariables\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         )\n\u001b[0;32m--> 145\u001b[0;31m         loaded = tf.__internal__.saved_model.load_partial(\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         loader = Loader(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0m\u001b[1;32m   1032\u001b[0m                         ckpt_options, options, filters)\n\u001b[1;32m   1033\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_node_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_ordered_node_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_skip_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_load_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_load_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;34m\"\"\"Loads all nodes and functions from the SavedModel and their edges.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_load_nodes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mnode_setters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslot_variable_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslot_variable_node_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetattr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mnode_setters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_recreate\u001b[0;34m(self, proto, node_id, nodes)\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_recreate_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_recreate_default\u001b[0;34m(self, proto, node_id, deps)\u001b[0m\n\u001b[1;32m    671\u001b[0m       raise ValueError(f\"Unknown SavedObject type: {kind}. Expected one of \"\n\u001b[1;32m    672\u001b[0m                        f\"{list(factory.keys())}.\")\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_recreate_user_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_bare_concrete_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             proto=proto.bare_concrete_function, dependencies=deps),\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;34m\"variable\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recreate_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m         \"captured_tensor\": functools.partial(\n\u001b[1;32m    667\u001b[0m             self._get_tensor_from_fn, proto.captured_tensor),\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_recreate_variable\u001b[0;34m(self, proto)\u001b[0m\n\u001b[1;32m    787\u001b[0m               aggregation=aggregation), setattr\n\u001b[1;32m    788\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         return variables.Variable(\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_variable_call\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_call\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0mvariable_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvariable_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariable_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape, experimental_enable_variable_lifting, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maggregation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0maggregation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     return previous_getter(\n\u001b[0m\u001b[1;32m   1228\u001b[0m         \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mgetter\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcaptured_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36muninitialized_variable_creator\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    764\u001b[0m       \u001b[0;34m\"\"\"A variable creator that creates uninitialized variables.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUninitializedVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;31m# Create a variable_creator_scope that creates uninitialized variables with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvariable_call\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariable_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, caching_device, name, shape, dtype, constraint, synchronization, aggregation, extra_handle_data, distribute_strategy, **unused_kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m           \u001b[0munique_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m           \u001b[0mshared_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Never shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m         handle = _variable_handle_from_shape_and_dtype(\n\u001b[0m\u001b[1;32m   2270\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[0;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mshared_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m   handle = gen_resource_variable_ops.var_handle_op(\n\u001b[0m\u001b[1;32m    171\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mvar_handle_op\u001b[0;34m(dtype, shape, container, shared_name, allowed_devices, name)\u001b[0m\n\u001b[1;32m   1262\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VarHandleOp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"container\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shared_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mshared_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"allowed_devices\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Hub version: \", hub.__version__) #TensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere\n",
        "print(\"Eager mode: \", tf.executing_eagerly()) #TensorFlow calculates the values of tensors as they occur in your code\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_root='/content/drive/MyDrive/COURSE PROJECT';\n",
        "\n",
        "IMAGE_SHAPE = (224, 224)\n",
        "TRAINING_DATA_DIR = str(data_root)\n",
        "print(TRAINING_DATA_DIR);\n",
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"validation\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SHAPE\n",
        ")\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "TRAINING_DATA_DIR,\n",
        "subset=\"training\",\n",
        "shuffle=True,\n",
        "target_size=IMAGE_SHAPE)\n",
        "\n",
        "image_batch_train, label_batch_train = next(iter(train_generator))\n",
        "print(\"Image batch shape: \", image_batch_train.shape)\n",
        "print(\"Label batch shape: \", label_batch_train.shape)\n",
        "dataset_labels = sorted(train_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "dataset_labels = np.array([key.title() for key, value in dataset_labels])\n",
        "print(dataset_labels)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",\n",
        "                 output_shape=[1280],\n",
        "                 trainable=False),\n",
        "  tf.keras.layers.Dropout(0.4),\n",
        "  tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')\n",
        "])\n",
        "model.build([None, 224, 224, 3])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.Adam(),\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['acc'])\n",
        "\n",
        "steps_per_epoch = np.ceil(train_generator.samples/train_generator.batch_size)\n",
        "val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)\n",
        "\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=val_steps_per_epoch).history\n",
        "\n",
        "final_loss, final_accuracy = model.evaluate(valid_generator, steps = val_steps_per_epoch)\n",
        "print(\"Final loss: {:.2f}\".format(final_loss))\n",
        "print(\"Final accuracy: {:.2f}%\".format(final_accuracy * 100))\n",
        "\n",
        "saved_model_path = \"./my_model.h5\" # or you can simply use 'my_mode.h5'\n",
        "model.save('/content/drive/MyDrive/LAmodelData') #save your model\n",
        "\n",
        "saved_model_path = \"saved_models/LearnerActions\" # or you can simply use 'my_mode.h5'\n",
        "model.save(saved_model_path) #save your model\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "# to reload your model\n",
        "shoe_model = keras.models.load_model(saved_model_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWD8WC_5C4om"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "###############################################\n",
        "###############################################\n",
        "####### Get images and labels batch THAT NEEDS TO BE CATEGORIZED\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "# Load the pre-trained model\n",
        "shoe_model = keras.models.load_model(saved_model_path)\n",
        "\n",
        "# Directory containing new images\n",
        "new_images_directory = \"/content/drive/MyDrive/v2_sample1\"\n",
        "\n",
        "# Load and preprocess images from the directory\n",
        "image_files = os.listdir(new_images_directory)\n",
        "images = [cv2.imread(os.path.join(new_images_directory, file)) for file in image_files]\n",
        "images = [cv2.resize(img, (224, 224)) for img in images]  # Resize to match your model's input size\n",
        "images = np.array(images) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Predict using the model\n",
        "tf_model_predictions = shoe_model.predict(images)\n",
        "print(\"Prediction results shape:\", tf_model_predictions.shape)\n",
        "\n",
        "# Create a directory to save individual images\n",
        "output_directory = 'downloaded_images'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "# Save each image with its ID as the filename in high resolution\n",
        "for i, prediction in enumerate(tf_model_predictions):\n",
        "    image_filename = f\"{output_directory}/image_{i + 1}_predicted_{np.argmax(prediction)}.png\"\n",
        "    plt.imshow(images[i])\n",
        "    plt.axis('off')\n",
        "    plt.savefig(image_filename, dpi=300, bbox_inches='tight')\n",
        "    plt.clf()  # Clear the figure to avoid overlapping images\n",
        "\n",
        "# Create a ZIP file containing the individual images\n",
        "import shutil\n",
        "shutil.make_archive(output_directory, 'zip', output_directory)\n",
        "\n",
        "# Download the ZIP file\n",
        "from google.colab import files\n",
        "files.download(f'{output_directory}.zip')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load and preprocess images from the directory\n",
        "image_files = os.listdir(new_images_directory)\n",
        "images = [cv2.imread(os.path.join(new_images_directory, file)) for file in image_files]\n",
        "images = [cv2.resize(img, (224, 224)) for img in images]  # Resize to match your model's input size\n",
        "images = np.array(images) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Predict using the model\n",
        "tf_model_predictions = shoe_model.predict(images)\n",
        "print(\"Prediction results shape:\", tf_model_predictions.shape)\n",
        "\n",
        "\n",
        "tf_pred_dataframe = pd.DataFrame(tf_model_predictions)\n",
        "tf_pred_dataframe.columns = dataset_labels\n",
        "\n",
        "print(\"Prediction results for the first elements\")\n",
        "tf_pred_dataframe.head()\n",
        "\n",
        "\n",
        "predicted_ids = np.argmax(tf_model_predictions, axis=-1)\n",
        "predicted_labels = dataset_labels[predicted_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITRlyUtb9OGr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load and preprocess images from the directory\n",
        "image_files = os.listdir(new_images_directory)\n",
        "images = [cv2.imread(os.path.join(new_images_directory, file)) for file in image_files]\n",
        "images = [cv2.resize(img, (224, 224)) for img in images]  # Resize to match your model's input size\n",
        "images = np.array(images) / 255.0  # Normalize pixel values\n",
        "\n",
        "# Predict using the model\n",
        "tf_model_predictions = shoe_model.predict(images)\n",
        "print(\"Prediction results shape:\", tf_model_predictions.shape)\n",
        "\n",
        "# Get predicted labels\n",
        "predicted_ids = np.argmax(tf_model_predictions, axis=-1)\n",
        "predicted_labels = dataset_labels[predicted_ids]\n",
        "\n",
        "# ... (previous code remains unchanged)\n",
        "\n",
        "# Extract only the numeric part from the file names\n",
        "image_labels = [file.split('.')[0] for file in image_files]\n",
        "image_labels = [''.join(filter(str.isdigit, label)) for label in image_labels]\n",
        "\n",
        "# Create a DataFrame with Image Labels and Predicted Labels\n",
        "results_df = pd.DataFrame({\n",
        "    'Image_Label': image_labels,  # Extracting the numeric part from the filename\n",
        "    'Predicted_Label': predicted_labels\n",
        "})\n",
        "\n",
        "# Convert the 'Image_Label' column to integers for sorting\n",
        "results_df['Image_Label'] = results_df['Image_Label'].astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the 'Image_Label' column\n",
        "results_df = results_df.sort_values(by='Image_Label')\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('model_predictions_results.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "files.download('model_predictions_results.csv')\n",
        "\n",
        "\n",
        "\n",
        "###from here onwards pat of old code\n",
        "# Create a DataFrame with Image Labels and Predicted Labels\n",
        "#results_df = pd.DataFrame({\n",
        "#    'Image_Label': [file.split('.')[0] for file in image_files],  # Extracting the label from the filename\n",
        " #   'Predicted_Label': predicted_labels\n",
        "#})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "#results_df.to_csv('model_predictions_results.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "#from google.colab import files\n",
        "#files.download('model_predictions_results.csv')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR7S79hcxpsz"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "# Create a DataFrame with Image Labels, Predicted Labels, and Timestamps\n",
        "results_df = pd.DataFrame({\n",
        "    'Image_Label': image_labels,  # Using the sorted and renamed Image_Labels\n",
        "    'Predicted_Label': predicted_labels\n",
        "})\n",
        "\n",
        "# Convert the 'Image_Label' column to integers for sorting\n",
        "results_df['Image_Label'] = results_df['Image_Label'].astype(int)\n",
        "\n",
        "# Sort the DataFrame based on the 'Image_Label' column\n",
        "results_df = results_df.sort_values(by='Image_Label')\n",
        "\n",
        "# Add Timestamps based on the identified learner actions\n",
        "timestamps = []\n",
        "current_action = None\n",
        "start_time = None\n",
        "\n",
        "for index, row in results_df.iterrows():\n",
        "    if current_action is None:\n",
        "        current_action = row['Predicted_Label']\n",
        "        start_time = int(''.join(filter(str.isdigit, str(row['Image_Label']))))\n",
        "    elif current_action != row['Predicted_Label']:\n",
        "        end_time = int(''.join(filter(str.isdigit, str(row['Image_Label'])))) - 1\n",
        "        timestamps.append({'Action': current_action, 'Start_Time': start_time, 'End_Time': end_time})\n",
        "        current_action = row['Predicted_Label']\n",
        "        start_time = int(''.join(filter(str.isdigit, str(row['Image_Label']))))\n",
        "\n",
        "# Add the last action\n",
        "end_time = int(''.join(filter(str.isdigit, str(row['Image_Label']))))\n",
        "timestamps.append({'Action': current_action, 'Start_Time': start_time, 'End_Time': end_time})\n",
        "\n",
        "timestamps_df = pd.DataFrame(timestamps)\n",
        "timestamps_df.to_csv('learner_action_timestamps.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "from google.colab import files\n",
        "files.download('learner_action_timestamps.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bup6YWlhF8ie"
      },
      "outputs": [],
      "source": [
        "# Define a mapping of predicted labels to learner actions\n",
        "def map_to_learner_actions(predicted_label):\n",
        "    if predicted_label in [\"Jupyter Success\", \"Jupyter Error\", \"Jupyter Running\"]:\n",
        "        return \"Jupyter\"\n",
        "    elif predicted_label in [\"Gpt Writing\", \"Gpt Reading\", \"Gpt Generating\"]:\n",
        "        return \"Chat GPT\"\n",
        "    elif predicted_label == \"excel\":\n",
        "        return \"excel\"\n",
        "    elif predicted_label == \"other\":\n",
        "        return \"other\"\n",
        "    else:\n",
        "        return \"Unknown\"\n",
        "\n",
        "# Map the predicted labels to learner actions\n",
        "learner_actions = [map_to_learner_actions(label) for label in predicted_labels]\n",
        "\n",
        "# Extract and process image labels to use as time stamps\n",
        "# Note: Assuming 'image_labels' are already defined and sorted in your existing code\n",
        "time_stamps = [int(''.join(filter(str.isdigit, label))) for label in image_labels]\n",
        "\n",
        "# Create a DataFrame with Time Stamps, Learner Actions, and Predicted Labels\n",
        "results_df = pd.DataFrame({\n",
        "    'Time Stamp': time_stamps,               # The renamed first column\n",
        "    'Learner_Actions': learner_actions,      # The new second column with mapped actions\n",
        "    'Predicted_Label': predicted_labels      # The third column\n",
        "})\n",
        "\n",
        "# No need to convert 'Time Stamp' column to integers since it's already done above\n",
        "# Sort the DataFrame based on the 'Time Stamp' column, if needed\n",
        "results_df = results_df.sort_values(by='Time Stamp')\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('model_predictions_results.csv', index=False)\n",
        "\n",
        "# Download the CSV file\n",
        "#from google.colab import files\n",
        "#files.download('model_predictions_results.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYdvFUWhkYRF"
      },
      "outputs": [],
      "source": [
        "# Rename the 'Time Stamp' column to 'Image'\n",
        "results_df.rename(columns={'Time Stamp': 'Image'}, inplace=True)\n",
        "\n",
        "# Add another column for time stamps in seconds\n",
        "results_df['Time (Seconds)'] = results_df['Image'] * 0.2  # Assuming each image represents 0.2 seconds\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('model_predictions_results_updated.csv', index=False)\n",
        "\n",
        "# Download the updated CSV file\n",
        "from google.colab import files\n",
        "files.download('model_predictions_results_updated.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y4o0fVokd3x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}